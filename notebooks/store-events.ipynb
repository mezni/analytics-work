{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cc71f13-4bf4-47e9-bf7e-788714aa83ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def get_spark_session(config):\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(config['app_name']) \\\n",
    "        .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "        .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.4.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "        .config(\"spark.cassandra.connection.host\", config['store_host']) \\\n",
    "        .config(\"spark.cassandra.connection.port\",config['store_port'])\\\n",
    "        .config(\"spark.cassandra.auth.username\", config['store_user']) \\\n",
    "        .config(\"spark.cassandra.auth.password\", config['store_pass']) \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "        .master(config['spark_uri']) \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "config ={\n",
    "    \"app_name\": \"store events\",\n",
    "    \"store_host\": \"events-db\",\n",
    "    \"store_port\": \"9042\",\n",
    "    \"store_user\": \"cassandra\",\n",
    "    \"store_pass\": \"cassandra\",\n",
    "    \"spark_uri\": \"spark://spark-master:7077\",\n",
    "    \"kafka_bootstrap_servers\": \"kafka1:19092,kafka2:19093,kafka3:19094\",\n",
    "    \"kafka_topic\": \"events\"\n",
    "}\n",
    "spark = get_spark_session(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835095a-94ec-42ac-85a6-8fd88304ce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, LongType\n",
    "from pyspark.sql.functions import from_json, col, to_timestamp, date_format, lit, count\n",
    "\n",
    "events_schema = StructType([ \n",
    "    StructField('timestamp', StringType(), True),\n",
    "    StructField('type', StringType(), True),\n",
    "    StructField('appName', StringType(), True), \n",
    "    StructField('appInstance', LongType(), True),\n",
    "    StructField('appID', StringType(), True),\n",
    "    StructField('probeID', StringType(), True),\n",
    "    StructField('eventID', StringType(), True),\n",
    "    StructField('correletionID', LongType(), True),\n",
    "    StructField('locationID', StringType(), True),\n",
    "    StructField('transactionStart', LongType(), True), \n",
    "    StructField('transactionEnd', LongType(), True), \n",
    "    StructField('transactionDuration', LongType(), True), \n",
    "    StructField('clientIPAddress', StringType(), True),\n",
    "    StructField('clientPort', IntegerType(), True), \n",
    "    StructField('serverIPAddress', StringType(), True), \n",
    "    StructField('serverPort', IntegerType(), True), \n",
    "    StructField('ipProtocol', StringType(), True), \n",
    "    StructField('category', StringType(), True), \n",
    "    StructField('bytesFromClient', LongType(), True), \n",
    "    StructField('bytesToClient', LongType(), True), \n",
    "    StructField('bytesFromServer', LongType(), True), \n",
    "    StructField('bytesToServer', LongType(), True), \n",
    "    StructField('subscriberID', StringType(), True), \n",
    "    StructField('applicationProtocol', StringType(), True), \n",
    "    StructField('applicationName', StringType(), True), \n",
    "    StructField('domain', StringType(), True), \n",
    "    StructField('deviceType', StringType(), True), \n",
    "    StructField('networkType', StringType(), True), \n",
    "    StructField('contentType', StringType(), True), \n",
    "    StructField('lostBytesClient', LongType(), True), \n",
    "    StructField('lostBytesServer', LongType(), True), \n",
    "    StructField('srttMsClient', LongType(), True), \n",
    "    StructField('srttMsServer', LongType(), True), \n",
    "])\n",
    "\n",
    "\n",
    "def init_stream(spark, config):    \n",
    "    df = spark.readStream.format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", config['kafka_bootstrap_servers']) \\\n",
    "        .option(\"subscribe\", config['kafka_topic']) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"failOnDataLoss\",\"FALSE\") \\\n",
    "        .load() \n",
    "    return df\n",
    "\n",
    "def process_df(df):\n",
    "    batch_id = str(uuid.uuid4())\n",
    "    df = df.selectExpr(\"CAST(value AS STRING)\")\\\n",
    "        .select(from_json(col(\"value\"),events_schema)\\\n",
    "        .alias(\"data\"))\\\n",
    "        .withColumn(\"batchID\", lit(batch_id))\\\n",
    "        .select(date_format(to_timestamp(\"data.transactionEnd\"),\"yyyyMMddHHmm\").alias(\"bucket_id\")\\\n",
    "            ,col(\"batchID\").alias(\"batch_id\")\\\n",
    "            ,col(\"data.probeID\").alias(\"probe_id\")\\\n",
    "            ,col(\"data.eventID\").alias(\"event_id\")\\\n",
    "            ,col(\"data.locationID\").alias(\"location_id\")\\\n",
    "            ,col(\"data.transactionStart\").alias(\"transaction_start\")\\\n",
    "            ,col(\"data.transactionEnd\").alias(\"transaction_end\")\\\n",
    "            ,col(\"data.transactionDuration\").alias(\"transaction_duration\")\\\n",
    "            ,col(\"data.clientIPAddress\").alias(\"client_ip_address\")\\\n",
    "            ,col(\"data.clientPort\").alias(\"client_port\")\\\n",
    "            ,col(\"data.serverIPAddress\").alias(\"server_ip_address\")\\\n",
    "        )\n",
    "    return df\n",
    " \n",
    "init_df = init_stream(spark, config)\n",
    "df = process_df(init_df)\n",
    "#df1 = df.groupBy(\"batch_id\").agg(count(\"*\").alias(\"cnt\"))\n",
    "\n",
    "query = (df.writeStream\\\n",
    "            .outputMode(\"append\")\\\n",
    "            .format(\"console\")\\\n",
    "            .start())\n",
    "\n",
    "#query1 = (df1.writeStream\\\n",
    "#            .outputMode(\"append\")\\\n",
    "#            .format(\"console\")\\\n",
    "#            .start())\n",
    "\n",
    "\n",
    "while query.isActive:\n",
    "    print (query.lastProgress)\n",
    "    time.sleep(10)\n",
    "query.awaitTermination()\n",
    "#query1.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83198185-0641-4348-a11e-393c2241bc84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
